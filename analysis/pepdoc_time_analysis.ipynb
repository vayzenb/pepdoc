{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pdb\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'/lab_data/behrmannlab/claire/pepdoc/results_ex1' #read in the file; first value is the file name\n",
    "curr_dir = f'/user_data/vayzenbe/GitHub_Repos/pepdoc' #CHANGE AS NEEEDED CAUSE ITS FOR VLAAAD\n",
    "bin_size = 5 #20 ms bins (EACH BIN IS 4 MS SO 5 ROWS ARE 20 MS)\n",
    "# bin_size = 1 \n",
    "categories = ['tool','nontool','bird','insect']\n",
    "labels = np.asanyarray([0]*5 + [1]*5 + [2]*5 + [3]*5) #creates labels for data\n",
    "\n",
    "#d_channels\n",
    "channels = [77, 78, 79, 80, 86, 87, 88, 89, 98, 99, 100, 110, 109, 118, 131, 143, 154, 163, 130, 142, 153, 162, 129, 141, 152, 128, 140, 127] # a list of channels\n",
    "columns  =[f'E{ii}' for ii in channels] #convert channels into the same format as the columns\n",
    "\n",
    "#v_channels\n",
    "ventral_channels = [104, 105, 106, 111, 112, 113, 114, 115, 120, 121, 122, 123, 133, 134, 169, 177, 189, 159, 168, 176, 18, 199, 158, 167, 175, 187, 166, 174] # a list of channels\n",
    "ventral_columns  =[f'E{ii}' for ii in ventral_channels] #convert channels into the same format as the columns\n",
    "\n",
    "#c_channels\n",
    "control_channels =  [11, 12, 18, 19, 20, 21, 25, 26, 27, 32, 33, 34, 37, 38]\n",
    "control_columns  =[f'E{ii}' for ii in control_channels] #convert channels into the same format as the columns\n",
    "\n",
    "svm_test_size = .4\n",
    "svm_splits = 20\n",
    "sss = StratifiedShuffleSplit(n_splits=svm_splits, test_size=svm_test_size)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sub_decode =np.load(f'{curr_dir}/results/d_sub_decode.npy')\n",
    "v_sub_decode =np.load(f'{curr_dir}/results/v_sub_decode.npy')\n",
    "c_sub_decode =np.load(f'{curr_dir}/results/c_sub_decode.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Time Comparison'''\n",
    "#dorsal\n",
    "d_sub_decode = np.asanyarray(d_sub_decode)\n",
    "d_timecomp = []\n",
    "for i in range(0, d_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((d_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    d_timecomp.append(a)\n",
    "\n",
    "\n",
    "#ventral\n",
    "v_sub_decode = np.asanyarray(v_sub_decode)\n",
    "v_timecomp = []\n",
    "for i in range(0, v_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((v_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    v_timecomp.append(a)\n",
    "\n",
    "\n",
    "#control\n",
    "c_sub_decode = np.asanyarray(c_sub_decode)\n",
    "c_timecomp = []\n",
    "for i in range(0, c_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((c_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    c_timecomp.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESAMPLING\n",
    "\n",
    "#how many times to do the resampling\n",
    "#we'll eventually set this to somethign really high like 10,000\n",
    "iter = 100\n",
    "\n",
    "d_sub_decode_short = d_sub_decode[:, 9:]\n",
    "v_sub_decode_short = v_sub_decode[:, 9:]\n",
    "c_sub_decode_short = c_sub_decode[:, 9:]\n",
    "#subs = np.arange(0,20).reshape(20,1)\n",
    "#d_sub_decode_short = np.append(subs, d_sub_decode,1)\n",
    "#v_sub_decode_short = np.append(subs, v_sub_decode,1)\n",
    "#c_sub_decode_short = np.append(subs, c_sub_decode,1)\n",
    "#d_sub_decode_short = d_sub_decode\n",
    "\n",
    "#d_sub_decode_short = d_sub_decode\n",
    "\n",
    "#convert data to pandas dataframe\n",
    "#this is just because pandas has a good resampling function \n",
    "d_sub_decode_short = pd.DataFrame(d_sub_decode_short)\n",
    "v_sub_decode_short = pd.DataFrame(v_sub_decode_short)\n",
    "c_sub_decode_short = pd.DataFrame(c_sub_decode_short)\n",
    "\n",
    "#Create empty lists that will hold the results of each resample\n",
    "d_boot = []\n",
    "v_boot = []\n",
    "c_boot = []\n",
    "\n",
    "d_sub_counts = np.zeros((1,134))\n",
    "v_sub_counts = np.zeros((1,134))\n",
    "c_sub_counts = np.zeros((1,134))\n",
    "\n",
    "for ii in range(0,iter):\n",
    "    \n",
    "    #resample the sub decode data with replacement\n",
    "    d_sub_sample = d_sub_decode_short.sample(d_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    v_sub_sample = v_sub_decode_short.sample(v_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    c_sub_sample = c_sub_decode_short.sample(c_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    \n",
    "    #convert it back to a numpy array\n",
    "    d_sub_sample = d_sub_sample.to_numpy() \n",
    "    v_sub_sample = v_sub_sample.to_numpy()\n",
    "    c_sub_sample = c_sub_sample.to_numpy()\n",
    "\n",
    "    #calculate the bootstrap sample mean\n",
    "    d_timecomp = []\n",
    "    v_timecomp = []\n",
    "    c_timecomp = []\n",
    "    \n",
    "    for time in range(0,d_sub_sample.shape[1]):\n",
    "        d_stat= stats.ttest_1samp(d_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "        v_stat = stats.ttest_1samp(v_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "        c_stat = stats.ttest_1samp(c_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "\n",
    "        #append the p-value for every time point\n",
    "        d_timecomp.append(d_stat[1])  \n",
    "        v_timecomp.append(v_stat[1])\n",
    "        c_timecomp.append(c_stat[1])\n",
    "    \n",
    "    #reconvert p-value list into a numpy array\n",
    "    d_timecomp = np.asanyarray(d_timecomp)\n",
    "    v_timecomp = np.asanyarray(v_timecomp)\n",
    "    c_timecomp = np.asanyarray(c_timecomp)\n",
    "\n",
    "    \n",
    "    #find the the first time point that is below change (0.05)\n",
    "    #np.where simply returns the indices (i.e., spots in an array), that meet some condition\n",
    "    #i'm simply grabbing the first value of that list, which corresponds to the first time point above chance\n",
    "    d_onset = np.where(d_timecomp <.05,)[0][0]\n",
    "    v_onset = np.where(v_timecomp <.05,)[0][0]\n",
    "    c_onset = np.where(c_timecomp <.05,)[0][0]\n",
    "\n",
    "    d_sub_counts[0,np.where(d_timecomp <.05)[0]] += 1\n",
    "    v_sub_counts[0,np.where(v_timecomp <.05)[0]] += 1\n",
    "    c_sub_counts[0,np.where(c_timecomp <.05)[0]] += 1\n",
    "\n",
    "    #if d_onset == 1:\n",
    "    #    pdb.set_trace()\n",
    "    \n",
    "    #convert to the actual time point\n",
    "    d_onset_converted = d_onset\n",
    "    v_onset_converted = v_onset\n",
    "    c_onset_converted = c_onset\n",
    "\n",
    "    #d_onset_converted = d_onset *4\n",
    "    #v_onset_converted = v_onset *4\n",
    "    #c_onset_converted = c_onset *4\n",
    "\n",
    "    #add the onset value from the resample to a list\n",
    "    d_boot.append(d_onset_converted)\n",
    "    v_boot.append(v_onset_converted)\n",
    "    c_boot.append(c_onset_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5019f625d8cef7bf561ee867ce6ef83187ba244587e1135dd2819ee04470ac03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

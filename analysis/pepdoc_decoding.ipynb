{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'/lab_data/behrmannlab/claire/pepdoc/results_ex1' #read in the file; first value is the file name\n",
    "bin_size = 1\n",
    "categories = ['tool','nontool','bird','insect']\n",
    "labels = np.asanyarray([0]*5 + [1]*5 + [2]*5 + [3]*5) #creates labels for data\n",
    "\n",
    "#d_channels = [128, 129, 130, 142, 141, 153, 152, 140]\n",
    "#v_channels = [92, 93, 94, 102, 103, 104, 91, 111]\n",
    "svm_test_size = .4\n",
    "svm_splits = 10\n",
    "sss = StratifiedShuffleSplit(n_splits=svm_splits, test_size=svm_test_size)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "\n",
    "#d_cols = [f'E{ii}' for ii in d_channels]\n",
    "#v_cols = [f'E{ii}' for ii in v_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load All Subjects Data\n",
    "'''\n",
    "\n",
    "'''\n",
    "d_channels = [130]\n",
    "v_channels = [104]\n",
    "d_cols = [f'E{ii}' for ii in d_channels]\n",
    "v_cols = [f'E{ii}' for ii in v_channels]\n",
    "'''\n",
    "\n",
    "all_subs = ['AC_newepoch','AM','BB','CM','GG','HA','ZZ']\n",
    "all_sub_data = []\n",
    "#all_dorsal =[]\n",
    "#all_ventral =[]\n",
    "\n",
    "for nSubject in all_subs: #loop through categories\n",
    "    all_data =[]\n",
    "    for category in categories: #loop through categories\n",
    "        for nn in range(1,6): #loop through exemplars in categories\n",
    "           \n",
    "            curr_df = pd.read_csv(f'/{data_dir}/{nSubject}/{category}s/{category}{nn}.tsv' , sep='\\t')#read in the file; first value is the file name\n",
    "            curr_df = curr_df.T #use pandas to transpose data\n",
    "            curr_df.columns = curr_df.iloc[0] #set the column names to the first row\n",
    "            curr_df = curr_df.drop(curr_df.index[0]) #drop the first row\n",
    "            curr_df = curr_df.astype(float) #convert to float\n",
    "\n",
    "            bin_data = curr_df.rolling(bin_size).mean() #rolling avg given the bin size\n",
    "            \n",
    "            bin_data = bin_data.dropna() #drop missing values\n",
    "            bin_data = bin_data.reset_index() #reset the index of the dataframe\n",
    "            \n",
    "            bin_data = bin_data.drop(columns = ['index']) #drop columns\n",
    "\n",
    "            #dorsal_data = bin_data[d_cols]\n",
    "            #ventral_data = bin_data[v_cols]\n",
    "\n",
    "            all_data.append(bin_data.to_numpy())\n",
    "            #all_dorsal.append(dorsal_data.to_numpy())\n",
    "            #all_ventral.append(ventral_data.to_numpy())\n",
    "            #all_subs.append(nSubject.split('_')[0])\n",
    "            \n",
    "          \n",
    "    all_data = np.asanyarray(all_data) # the error \"ValueError: could not broadcast input array from shape (138,240) into shape (138,)\" is because the data is not the same length for all participants/error in adding participants.\n",
    "    #all_dorsal = np.asanyarray(all_dorsal)\n",
    "    #all_ventral = np.asanyarray(all_ventral)  \n",
    "    all_sub_data.append(all_data) #add the subject to the list of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dorsal_data.columns:\n",
    "    plt.plot(bin_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decode from all channels\n",
    "\"\"\"\n",
    "#empty list to hold acc\n",
    "cat_decode = []\n",
    "for time in range(0, all_data.shape[1]):\n",
    "    \n",
    "    X = all_data[:,time,:] #grab all data for that time point\n",
    "    y = labels #set Y to be the labels\n",
    "\n",
    "    temp_acc = [] #create empty list accuracy for each timepoint\n",
    "    for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index] #\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(X_train, y_train)   \n",
    "\n",
    "        temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "noICA = cat_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_lsit =['ca',3,'top']\n",
    " \n",
    "some_lsit[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data[0:7] #but I want to call one subject at a time and do the decoding\n",
    "#all_data[0] # and then 1 and then 2 and then 3 and then 4 and then 5 and then 6 and then 7...\n",
    "\n",
    "\n",
    "for x in all_sub_data:\n",
    "    print(x.shape)#print the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_decode in all_sub_data: # refer to a numpy array - by indexing an item in a list you can access the numpy array\n",
    "    sub_decode = all_sub_data.index(sub_decode)\n",
    "    print (sub_decode.shape)\n",
    "\n",
    "print(nSubject)\n",
    "\n",
    "#x = all_data[1]\n",
    "\n",
    "# to call a specific item in a list, you can use the indexing operator: list[index] or list[index1:index2]\n",
    "\n",
    "all_data[0:7]\n",
    "print(all_data[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing: Decode from all channels\n",
    "\"\"\"\n",
    "all_sub_decode = []\n",
    "\n",
    "for sub_decode in all_sub_data: # for loop print each iteration of the list (each column)\n",
    "    #empty list to hold acc\n",
    "    cat_decode = []\n",
    "    for time in range(0, sub_decode.shape[1]):\n",
    "        \n",
    "        X = sub_decode[:,time,:] #grab all data for that time point\n",
    "        y = labels #set Y to be the labels\n",
    "        \n",
    "        temp_acc = [] #create empty list accuracy for each timepoint\n",
    "        for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index] #\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "            clf.fit(X_train, y_train)   \n",
    "\n",
    "            temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "    cat_decode = np.asanyarray(cat_decode)\n",
    "    all_sub_decode.append(cat_decode)\n",
    "    #noICA = cat_decode\n",
    "    #then add all accuracies to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Decode from dorsal channels\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cat_decode = []\n",
    "for time in range(0, all_dorsal.shape[1]):\n",
    "    \n",
    "    X = all_dorsal[:,time,:]\n",
    "    y = labels\n",
    "\n",
    "    temp_acc = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(X_train, y_train)   \n",
    "\n",
    "        temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "dorsal = cat_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = list(range(0, 500,4))\n",
    "plt.plot(timepoints, wICA)\n",
    "plt.plot(timepoints, noICA)\n",
    "plt.axhline(y=0.25, color='k', linestyle='--')\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data['E1'][0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cf590ee874f6c19d45d293ebdf4bde7f892b798462cd329a94381daf42f8eda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5019f625d8cef7bf561ee867ce6ef83187ba244587e1135dd2819ee04470ac03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

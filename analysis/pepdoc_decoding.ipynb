{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = f'/lab_data/behrmannlab/claire/pepdoc/results_ex1'\n",
    "bin_size = 1\n",
    "categories = ['tool','nontool','bird','insect']\n",
    "#categories = ['tool','nontool']\n",
    "labels = np.asanyarray([0]*5 + [1]*5 + [2]*5 + [3]*5) #creates labels for data\n",
    "\n",
    "d_channels = [128, 129, 130, 142, 141, 153, 152, 140]\n",
    "v_channels = [92, 93, 94, 102, 103, 104, 91, 111]\n",
    "svm_test_size = .4\n",
    "svm_splits = 10\n",
    "sss = StratifiedShuffleSplit(n_splits=svm_splits, test_size=svm_test_size)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "\n",
    "\n",
    "d_cols = [f'E{ii}' for ii in d_channels]\n",
    "v_cols = [f'E{ii}' for ii in v_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads data for each category\n",
    "'''\n",
    "\n",
    "all_subs = []#CLAIRE WILL MAKE A LOOP ACROSS SUBJECTS THAT WILL ADD THE DATA TO ONE LIST\n",
    "#make empty lists to hold data\n",
    "all_data =[]\n",
    "all_dorsal =[]\n",
    "all_ventral =[]\n",
    "for category in categories: #loop through categories\n",
    "    \n",
    "    for nn in range(1,6): #loop through exemplars in categories\n",
    "\n",
    "        curr_df = pd.read_csv(f'{data_dir}/{category}s/{category}{nn}', sep='\\t') #read in the file; first value is the file name\n",
    "\n",
    "        bin_data = curr_df.rolling(bin_size).mean() #rolling avg given the bin size\n",
    "        \n",
    "        bin_data = bin_data.dropna() #drop missing values\n",
    "        bin_data = bin_data.reset_index() #reset the index of the dataframe\n",
    "        bin_data = bin_data.drop(columns = ['index','Time']) #drop columns\n",
    "        dorsal_data = bin_data[d_cols]\n",
    "        ventral_data = bin_data[v_cols]\n",
    "\n",
    "        all_data.append(bin_data.to_numpy())\n",
    "        all_dorsal.append(dorsal_data.to_numpy())\n",
    "        all_ventral.append(ventral_data.to_numpy())\n",
    "\n",
    "all_data = np.asanyarray(all_data)\n",
    "all_dorsal = np.asanyarray(all_dorsal)\n",
    "all_ventral = np.asanyarray(all_ventral)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_channels = [130, 142, 141, 153, 152]\n",
    "v_channels = [104]\n",
    "d_cols = [f'E{ii}' for ii in d_channels]\n",
    "v_cols = [f'E{ii}' for ii in v_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sandbox for testing\n",
    "'''\n",
    "\n",
    "all_subs = ['AC_newepoch','AM','BB','CM','GG','HA','ZZ']\n",
    "data_dir = f'/lab_data/behrmannlab/claire/pepdoc/results_ex1' #read in the file; first value is the file name\n",
    "'''\n",
    "d_channels = [130]\n",
    "v_channels = [104]\n",
    "d_cols = [f'E{ii}' for ii in d_channels]\n",
    "v_cols = [f'E{ii}' for ii in v_channels]\n",
    "'''\n",
    "\n",
    "all_sub_data = []\n",
    "\n",
    "all_dorsal =[]\n",
    "all_ventral =[]\n",
    "print(data_dir)\n",
    "\n",
    "for nSubject in all_subs: #loop through categories\n",
    "    all_data =[]\n",
    "    for category in categories: #loop through categories\n",
    "        for nn in range(1,6): #loop through exemplars in categories\n",
    "            print(f'{nSubject} {category} {nn}')\n",
    "            curr_df = pd.read_csv(f'/{data_dir}/{nSubject}/{category}s/{category}{nn}.tsv' , sep='\\t')#read in the file; first value is the file name\n",
    "            curr_df = curr_df.T #use pandas to transpose data\n",
    "            curr_df.columns = curr_df.iloc[0] #set the column names to the first row\n",
    "            curr_df = curr_df.drop(curr_df.index[0]) #drop the first row\n",
    "            curr_df = curr_df.astype(float) #convert to float\n",
    "\n",
    "            bin_data = curr_df.rolling(bin_size).mean() #rolling avg given the bin size\n",
    "            \n",
    "            bin_data = bin_data.dropna() #drop missing values\n",
    "            bin_data = bin_data.reset_index() #reset the index of the dataframe\n",
    "            \n",
    "            bin_data = bin_data.drop(columns = ['index']) #drop columns\n",
    "\n",
    "            dorsal_data = bin_data[d_cols]\n",
    "            ventral_data = bin_data[v_cols]\n",
    "\n",
    "            all_data.append(bin_data.to_numpy())\n",
    "            #all_dorsal.append(dorsal_data.to_numpy())\n",
    "            #all_ventral.append(ventral_data.to_numpy())\n",
    "            #all_subs.append(nSubject.split('_')[0])\n",
    "            \n",
    "          \n",
    "    all_data = np.asanyarray(all_data) # the error \"ValueError: could not broadcast input array from shape (138,240) into shape (138,)\" is because the data is not the same length for all participants/error in adding participants.\n",
    "    #all_dorsal = np.asanyarray(all_dorsal)\n",
    "    #all_ventral = np.asanyarray(all_ventral)  \n",
    "    all_sub_data.append(all_data) #add the subject to the list\n",
    "#this chunk of script runs when I comment out all_data = np.asanyarray(all_data) or when I use only one subject at a time.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 138, 203)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sub_data[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transpose the data\n",
    "'''\n",
    "\n",
    "all_subs = ['AC_newepoch','AM','BB','CM','GG','HA','ZZ']\n",
    "data_dir = (f'lab_data/behrmannlab/claire/pepdoc/results_ex1') #read in the file; first value is the file name\n",
    "\n",
    "for nSubject in all_subs: #loop through categories\n",
    "    for category in categories: #loop through categories\n",
    "        for nn in range(1,6): #loop through exemplars in categories\n",
    "            curr_df = pd.read_csv(f'/{data_dir}/{nSubject}/{category}s/{category}{nn}.tsv' , sep='\\t')  #read in the file; first value is the file name\n",
    "            curr_df = curr_df.T #use pandas to transpose data\n",
    "\n",
    "print(curr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "absolute path\n",
    "'''\n",
    "\n",
    "all_subs = ['AC_newepoch','AM','BB','CM','GG','HA','ZZ']\n",
    "data_dir = (f'lab_data/behrmannlab/claire/pepdoc/results_ex1') #read in the file; first value is the file name\n",
    "\n",
    "for nSubject in all_subs: #loop through categories\n",
    "    for category in categories: #loop through categories\n",
    "        for nn in range(1,6): #loop through exemplars in categories\n",
    "            curr_df = pd.read_csv(f'/{data_dir}/{nSubject}/{category}s/{category}{nn}.tsv' , sep='\\t')  #read in the file; first value is the file name\n",
    "\n",
    "print(curr_df)\n",
    "print(nSubject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Practice\n",
    "\"\"\"\n",
    "\n",
    "data_dir = f'lab_data/behrmannlab/claire/pepdoc/results_ex1'\n",
    "# data_dir = f'/Users/clairesimmons/behrmannlab/pepdoc/results_ex1'\n",
    "bin_size = 1\n",
    "categories = ['tool','nontool','bird','insect']\n",
    "labels = np.asanyarray([0]*5 + [1]*5 + [2]*5 + [3]*5)\n",
    "\n",
    "for nSubject in all_subs: #loop through subjects\n",
    "    print(nSubject)\n",
    "    \n",
    "for nSubject in all_subs: #loop through subjects\n",
    "    print(f'{data_dir}/{nSubject}/{category}s/{category}{nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dorsal_data.columns:\n",
    "    plt.plot(bin_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decode from all channels\n",
    "\"\"\"\n",
    "#empty list to hold acc\n",
    "cat_decode = []\n",
    "for time in range(0, all_data.shape[1]):\n",
    "    \n",
    "    X = all_data[:,time,:] #grab all data for that time point\n",
    "    y = labels #set Y to be the labels\n",
    "    pdb.set_trace()\n",
    "\n",
    "    temp_acc = [] #create empty list accuracy for each timepoint\n",
    "    for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index] #\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(X_train, y_train)   \n",
    "\n",
    "        temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "noICA = cat_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing: Decode from all channels\n",
    "\"\"\"\n",
    "all_sub_data # for loop print each iteration of the list (each column)\n",
    "    #empty list to hold acc\n",
    "    cat_decode = []\n",
    "    for time in range(0, all_data.shape[1]):\n",
    "        \n",
    "        X = all_data[:,time,:] #grab all data for that time point\n",
    "        y = labels #set Y to be the labels\n",
    "        pdb.set_trace()\n",
    "\n",
    "        temp_acc = [] #create empty list accuracy for each timepoint\n",
    "        for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index] #\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #pdb.set_trace()\n",
    "            clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "            clf.fit(X_train, y_train)   \n",
    "\n",
    "            temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "    noICA = cat_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decode from dorsal channels\n",
    "\"\"\"\n",
    "\n",
    "cat_decode = []\n",
    "for time in range(0, all_dorsal.shape[1]):\n",
    "    \n",
    "    X = all_dorsal[:,time,:]\n",
    "    y = labels\n",
    "\n",
    "    temp_acc = []\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "        clf.fit(X_train, y_train)   \n",
    "\n",
    "        temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "    cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "dorsal = cat_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = list(range(0, 500,4))\n",
    "plt.plot(timepoints, wICA)\n",
    "plt.plot(timepoints, noICA)\n",
    "plt.axhline(y=0.25, color='k', linestyle='--')\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_data['E1'][0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cf590ee874f6c19d45d293ebdf4bde7f892b798462cd329a94381daf42f8eda"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5019f625d8cef7bf561ee867ce6ef83187ba244587e1135dd2819ee04470ac03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

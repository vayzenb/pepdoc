{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pdb\n",
    "import scipy.stats as stats\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'/lab_data/behrmannlab/claire/pepdoc/results_ex1' #read in the file; first value is the file name\n",
    "bin_size = 5 #20 ms bins (EACH BIN IS 4 MS SO 5 ROWS ARE 20 MS)\n",
    "# bin_size = 1 \n",
    "categories = ['tool','nontool','bird','insect']\n",
    "labels = np.asanyarray([0]*5 + [1]*5 + [2]*5 + [3]*5) #creates labels for data\n",
    "\n",
    "#d_channels\n",
    "dorsal_channels = [77, 78, 79, 80, 86, 87, 88, 89, 98, 99, 100, 110, 109, 118, 131, 143, 154, 163, 130, 142, 153, 162, 129, 141, 152, 128, 140, 127] # a list of channels\n",
    "dorsal_columns  =[f'E{ii}' for ii in dorsal_channels] #convert channels into the same format as the columns\n",
    "\n",
    "#v_channels\n",
    "ventral_channels = [104, 105, 106, 111, 112, 113, 114, 115, 120, 121, 122, 123, 133, 134, 169, 177, 189, 159, 168, 176, 18, 199, 158, 167, 175, 187, 166, 174] # a list of channels\n",
    "ventral_columns  =[f'E{ii}' for ii in ventral_channels] #convert channels into the same format as the columns\n",
    "\n",
    "#c_channels\n",
    "control_channels =  [11, 12, 18, 19, 20, 21, 25, 26, 27, 32, 33, 34, 37, 38]\n",
    "control_columns  =[f'E{ii}' for ii in control_channels] #convert channels into the same format as the columns\n",
    "\n",
    "svm_test_size = .4\n",
    "svm_splits = 20\n",
    "sss = StratifiedShuffleSplit(n_splits=svm_splits, test_size=svm_test_size)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load All Subjects Data\n",
    "'''\n",
    "all_subs = ['AC_newepoch','AM', 'BB','CM','CR','GG','HA','IB','JM','JR','KK','KT','MC','MH','NF','SB','SG','SOG','TL','ZZ']\n",
    "all_sub_data = []\n",
    "all_sub_dorsal =[]\n",
    "all_sub_ventral =[]\n",
    "all_sub_control = []\n",
    "\n",
    "for nSubject in all_subs: #loop through subs\n",
    "    all_data =[]\n",
    "    all_ventral =[]\n",
    "    all_dorsal =[]\n",
    "    all_control = []\n",
    "    for category in categories: #loop through categories\n",
    "        for nn in range(1,6): #loop through exemplars in categories\n",
    "           \n",
    "            curr_df = pd.read_csv(f'/{data_dir}/{nSubject}/{category}s/{category}{nn}.tsv' , sep='\\t')#read in the file; first value is the file name\n",
    "            curr_df = curr_df.T #use pandas to transpose data\n",
    "            curr_df.columns = curr_df.iloc[0] #set the column names to the first row\n",
    "            curr_df = curr_df.drop(curr_df.index[0]) #drop the first row\n",
    "            curr_df = curr_df.astype(float) #convert to float\n",
    "\n",
    "            bin_data = curr_df.rolling(bin_size).mean() #rolling avg given the bin size\n",
    "            \n",
    "            bin_data = bin_data.dropna() #drop missing values\n",
    "            bin_data = bin_data.reset_index() #reset the index of the dataframe\n",
    "            \n",
    "            all_channel_data = bin_data.drop(columns = ['index']) #drop columns that are not channels\n",
    "\n",
    "            dorsal_df = pd.DataFrame() #create empty data frame that you will populate with only those channels you care about\n",
    "            for ii in dorsal_columns: #loop through all channels of interest\n",
    "                if ii in all_channel_data.columns: #check if current channel exists in df\n",
    "                    dorsal_df[ii] = all_channel_data[ii] #if it does add it the empty one\n",
    "\n",
    "            ventral_df = pd.DataFrame() #create empty data frame that you will populate with only those channels you care about\n",
    "            for ii in ventral_columns: #loop through all channels of interest\n",
    "                if ii in all_channel_data.columns: #check if current channel exists in df\n",
    "                    ventral_df[ii] = all_channel_data[ii] #if it does add it the empty one\n",
    "\n",
    "\n",
    "            control_df = pd.DataFrame() #create empty data frame that you will populate with only those channels you care about\n",
    "            for ii in control_columns: #loop through all channels of interest\n",
    "                if ii in all_channel_data.columns: #check if current channel exists in df\n",
    "                    control_df[ii] = all_channel_data[ii] #if it does add it the empty one\n",
    "\n",
    "            all_data.append(all_channel_data.to_numpy())\n",
    "            all_dorsal.append(dorsal_df.to_numpy())\n",
    "            all_ventral.append(ventral_df.to_numpy())\n",
    "            all_control.append(control_df.to_numpy())\n",
    "            #all_subs.append(nSubject.split('_')[0])\n",
    "            \n",
    "          \n",
    "    all_data = np.asanyarray(all_data) # the error \"ValueError: could not broadcast input array from shape (138,240) into shape (138,)\" is because the data is not the same length for all participants/error in adding participants.\n",
    "    all_dorsal = np.asanyarray(all_dorsal)\n",
    "    all_ventral = np.asanyarray(all_ventral)  \n",
    "    all_control = np.asanyarray(all_control)\n",
    "\n",
    "    all_sub_data.append(all_data) #add the subject to the list of subjects\n",
    "    all_sub_dorsal.append(all_dorsal)\n",
    "    all_sub_ventral.append(all_ventral)\n",
    "    all_sub_control.append(all_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decode from all dorsal channels\n",
    "'''\n",
    "\n",
    "d_sub_decode = []\n",
    "\n",
    "for dorsal_sub_decode in all_sub_dorsal: # for loop print each iteration of the list (each column)\n",
    "    #empty list to hold acc\n",
    "    dorsal_cat_decode = []\n",
    "    for time in range(0, dorsal_sub_decode.shape[1]):\n",
    "        X = dorsal_sub_decode[:,time,:] #grab all data for that time point\n",
    "        y = labels #set Y to be the labels\n",
    "        \n",
    "        temp_acc = [] #create empty list accuracy for each timepoint\n",
    "        for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index] \n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "            clf.fit(X_train, y_train)   \n",
    "\n",
    "            temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        dorsal_cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "    dorsal_cat_decode = np.asanyarray(dorsal_cat_decode)\n",
    "    d_sub_decode.append(dorsal_cat_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decode from all ventral channels\n",
    "'''\n",
    "\n",
    "v_sub_decode = []\n",
    "\n",
    "for ventral_sub_decode in all_sub_ventral: # for loop print each iteration of the list (each column)\n",
    "    #empty list to hold acc\n",
    "    ventral_cat_decode = []\n",
    "    for time in range(0, ventral_sub_decode.shape[1]):\n",
    "        X = ventral_sub_decode[:,time,:] #grab all data for that time point\n",
    "        y = labels #set Y to be the labels\n",
    "        \n",
    "        temp_acc = [] #create empty list accuracy for each timepoint\n",
    "        for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "            clf.fit(X_train, y_train)   \n",
    "\n",
    "            temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        ventral_cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "    ventral_cat_decode = np.asanyarray(ventral_cat_decode)\n",
    "    v_sub_decode.append(ventral_cat_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decode from all control channels\n",
    "'''\n",
    "\n",
    "c_sub_decode = []\n",
    "\n",
    "for control_sub_decode in all_sub_control: # for loop print each iteration of the list (each column)\n",
    "    #empty list to hold acc\n",
    "    control_cat_decode = []\n",
    "    for time in range(0, control_sub_decode.shape[1]):\n",
    "        X = control_sub_decode[:,time,:] #grab all data for that time point\n",
    "        y = labels #set Y to be the labels\n",
    "        \n",
    "        temp_acc = [] #create empty list accuracy for each timepoint\n",
    "        for train_index, test_index in sss.split(X, y): #grab indices for training and test\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "            clf.fit(X_train, y_train)   \n",
    "\n",
    "            temp_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        control_cat_decode.append(np.mean(temp_acc))\n",
    "\n",
    "    control_cat_decode = np.asanyarray(control_cat_decode)\n",
    "    c_sub_decode.append(control_cat_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Time Comparison'''\n",
    "#dorsal\n",
    "d_sub_decode = np.asanyarray(d_sub_decode)\n",
    "d_timecomp = []\n",
    "for i in range(0, d_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((d_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    d_timecomp.append(a)\n",
    "\n",
    "\n",
    "#ventral\n",
    "v_sub_decode = np.asanyarray(v_sub_decode)\n",
    "timecomp_v = []\n",
    "for i in range(0, v_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((v_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    timecomp_v.append(a)\n",
    "\n",
    "\n",
    "#control\n",
    "c_sub_decode = np.asanyarray(c_sub_decode)\n",
    "timecomp_c = []\n",
    "for i in range(0, c_sub_decode.shape[1]):\n",
    "    a = stats.ttest_1samp((c_sub_decode[:,i]), .25, axis = 0, alternative='greater')\n",
    "    timecomp_c.append(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "timepoints = list(range(-36, 500, 4))\n",
    "fig = figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.axhline(.95, color = 'k', label='significant', linestyle='--')\n",
    "\n",
    "d_timecomp = np.asanyarray(d_timecomp)\n",
    "plt.plot(timepoints, 1-(d_timecomp[:,1]), label = 'dorsal')\n",
    "\n",
    "v_timecomp = np.asanyarray(timecomp_v)\n",
    "plt.plot(timepoints, 1-(v_timecomp[:,1]), label = 'ventral')\n",
    "\n",
    "c_timecomp = np.asanyarray(timecomp_c)\n",
    "plt.plot(timepoints, 1-(c_timecomp[:,1]), label = 'control')\n",
    "\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = list(range(-36, 500, 4))\n",
    "plt.xlabel('time (ms)')\n",
    "plt.axhline(.95, color = 'k', label='significant', linestyle='--')\n",
    "plt.axvline(90, color = 'k', label='significant', linestyle='--')\n",
    "\n",
    "d_timecomp = np.asanyarray(d_timecomp)\n",
    "plt.plot(timepoints, 1-(d_timecomp[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = list(range(-36, 500, 4))\n",
    "plt.xlabel('time (ms)')\n",
    "plt.axhline(.95, color = 'k', label='significant', linestyle='--')\n",
    "plt.axvline(101, color = 'k', label='significant', linestyle='--')\n",
    "\n",
    "v_timecomp = np.asanyarray(timecomp_v)\n",
    "plt.plot(timepoints, 1-(v_timecomp[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = list(range(-36, 500, 4))\n",
    "plt.xlabel('time (ms)')\n",
    "plt.axhline(.95, color = 'k', label='significant', linestyle='--')\n",
    "plt.axvline(170, color = 'k', label='significant', linestyle='--')\n",
    "\n",
    "c_timecomp = np.asanyarray(timecomp_c)\n",
    "plt.plot(timepoints, 1-(c_timecomp[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESAMPLING\n",
    "\n",
    "#how many times to do the resampling\n",
    "#we'll eventually set this to somethign really high like 10,000\n",
    "iter = 100\n",
    "\n",
    "#d_sub_decode_short = d_sub_decode[:, 9:]\n",
    "#v_sub_decode_short = v_sub_decode[:, 9:]\n",
    "#c_sub_decode_short = c_sub_decode[:, 9:]\n",
    "\n",
    "d_sub_decode_short = d_sub_decode\n",
    "v_sub_decode_short = v_sub_decode\n",
    "c_sub_decode_short = c_sub_decode\n",
    "\n",
    "#convert data to pandas dataframe\n",
    "#this is just because pandas has a good resampling function \n",
    "d_sub_decode_short = pd.DataFrame(d_sub_decode_short)\n",
    "v_sub_decode_short = pd.DataFrame(v_sub_decode_short)\n",
    "c_sub_decode_short = pd.DataFrame(c_sub_decode_short)\n",
    "\n",
    "#Create empty lists that will hold the results of each resample\n",
    "d_boot = []\n",
    "v_boot = []\n",
    "c_boot = []\n",
    "\n",
    "for ii in range(0,iter):\n",
    "    \n",
    "    #resample the sub decode data with replacement\n",
    "    d_sub_sample = d_sub_decode_short.sample(d_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    v_sub_sample = v_sub_decode_short.sample(v_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    c_sub_sample = c_sub_decode_short.sample(c_sub_decode_short.shape[0],replace = True, random_state=ii)\n",
    "    \n",
    "    #convert it back to a numpy array\n",
    "    d_sub_sample = d_sub_sample.to_numpy() \n",
    "    v_sub_sample = v_sub_sample.to_numpy()\n",
    "    c_sub_sample = c_sub_sample.to_numpy()\n",
    "\n",
    "    #calculate the bootstrap sample mean\n",
    "    d_timecomp = []\n",
    "    v_timecomp = []\n",
    "    c_timecomp = []\n",
    "\n",
    "    for time in range(0,d_sub_sample.shape[1]):\n",
    "        d_stat= stats.ttest_1samp(d_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "        v_stat = stats.ttest_1samp(v_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "        c_stat = stats.ttest_1samp(c_sub_sample[:,time], .25, axis = 0, alternative='greater')\n",
    "\n",
    "        #append the p-value for every time point\n",
    "        d_timecomp.append(d_stat[1])  \n",
    "        v_timecomp.append(v_stat[1])\n",
    "        c_timecomp.append(c_stat[1])\n",
    "    \n",
    "    #reconvert p-value list into a numpy array\n",
    "    d_timecomp = np.asanyarray(d_timecomp)\n",
    "    v_timecomp = np.asanyarray(v_timecomp)\n",
    "    c_timecomp = np.asanyarray(c_timecomp)\n",
    "    \n",
    "    #find the the first time point that is below change (0.05)\n",
    "    #np.where simply returns the indices (i.e., spots in an array), that meet some condition\n",
    "    #i'm simply grabbing the first value of that list, which corresponds to the first time point above chance\n",
    "    d_onset = np.where(d_timecomp <.05,)[0][0]\n",
    "    v_onset = np.where(v_timecomp <.05,)[0][0]\n",
    "    c_onset = np.where(c_timecomp <.05,)[0][0]\n",
    "    \n",
    "    #convert to the actual time point\n",
    "    d_onset_converted = d_onset\n",
    "    v_onset_converted = v_onset\n",
    "    c_onset_converted = c_onset\n",
    "\n",
    "    #d_onset_converted = d_onset *4\n",
    "    #v_onset_converted = v_onset *4\n",
    "    #c_onset_converted = c_onset *4\n",
    "\n",
    "    #add the onset value from the resample to a list\n",
    "    d_boot.append(d_onset_converted)\n",
    "    v_boot.append(v_onset_converted)\n",
    "    c_boot.append(c_onset_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_boot, range = [0, 150], label='dorsal', color='b', alpha=.5, bins= 150, density=True)\n",
    "plt.hist(v_boot,range = [0, 150], label='ventral', color='orange', alpha=.5, bins= 150, density=True)\n",
    "plt.hist(c_boot,range = [0, 150], label='control', color='g', alpha=.5, bins= 150, density=True)\n",
    "\n",
    "plt.xlabel('time (ms)')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(d_boot, d_boot, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_boot = np.asanyarray(d_boot)\n",
    "v_boot = np.asanyarray(v_boot)\n",
    "\n",
    "diff = d_boot - v_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(diff))\n",
    "print(np.std(diff)/np.sqrt(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(diff,notch=True)\n",
    "plt.axhline(0, color = 'k', label='significant', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the difference values with confidence intervals\n",
    "alpha = .025\n",
    "ci_low = np.percentile(diff, alpha*100)\n",
    "ci_high= np.percentile(diff, 100-alpha*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ci_low)\n",
    "print(ci_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = '/user_data/csimmon2/git_repos/pepdoc/analysis/d_sub_decode.npy')\n",
    "\n",
    "np.save('d_sub_decode.npy', d_sub_decode)\n",
    "#d_sub_decode = np.load(d_sub_decode)\n",
    "np.save('v_sub_decode.npy', v_sub_decode)\n",
    "#v_sub_decode = np.load(v_sub_decode)\n",
    "np.save('c_sub_decode.npy', c_sub_decode)\n",
    "#c_sub_decode = np.load(c_sub_decode)\n",
    "np.save(\"d_boot.npy\", d_boot)\n",
    "#d_boot = np.load(d_boot)\n",
    "np.save(\"v_boot.npy\", v_boot)\n",
    "#v_boot = np.load(v_boot)\n",
    "np.save(\"c_boot.npy\", c_boot)\n",
    "#c_boot = np.load(c_boot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cf590ee874f6c19d45d293ebdf4bde7f892b798462cd329a94381daf42f8eda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
